{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import run_epoch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "from utils import set_seed, Logger, CSVBatchLogger, log_args, get_model, hinge_loss, split_data, check_args, get_subsampled_indices\n",
    "import numpy as np\n",
    "\n",
    "# get path\n",
    "p = '0.7'\n",
    "dataset = 'CUB'\n",
    "seed = 0\n",
    "model_name = 'best_wg_acc_model'\n",
    "main_dir = f\"/home/thien/research/pseudogroups/{dataset}/splitpgl_sweep_logs/\" \\\n",
    "                  f\"p{p}_wd0.0001_lr0.0001\"\n",
    "best_model_path = f\"{main_dir}/part2_oll-1rw_rgl_group_dro_p0.7_wd2e-05_lr0.0001_s0/{model_name}.pth\"\n",
    "data_path = f\"{main_dir}/part1_s{seed}/part1and2_data_p{p}\"\n",
    "\n",
    "best_model_path1 = f\"{main_dir}/part1_s{seed}/best_model.pth\"\n",
    "device = 'cuda:0'\n",
    "\n",
    "# load data splits\n",
    "data = torch.load(data_path)\n",
    "part1_data, part2_data = data['part1'], data['part2']\n",
    "batch_size = 32\n",
    "\n",
    "part1_loader = DataLoader(part1_data, shuffle=False, batch_size=batch_size, pin_memory=True)\n",
    "part2_loader = DataLoader(part2_data, shuffle=False, batch_size=batch_size, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model = torch.load(best_model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "model1 = torch.load(best_model_path1)\n",
    "model1.to(device)\n",
    "model1.eval()\n",
    "\n",
    "loader = part1_loader\n",
    "\n",
    "is_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "105it [00:16,  6.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# now run the model on the desired dataset\n",
    "from tqdm import tqdm\n",
    "with torch.set_grad_enabled(is_training):  # to make sure we don't save grad when val\n",
    "    for batch_idx, batch in tqdm(enumerate(loader)):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x, y, g, data_idx = batch\n",
    "        outputs = model(x)\n",
    "\n",
    "        # now log the desired stats\n",
    "        # Calculate stats -- get the prediction and compare with groundtruth -- save to output df\n",
    "        if batch_idx == 0:\n",
    "            acc_y_pred = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "            acc_y_true = y.detach().cpu().numpy()\n",
    "            acc_g_true = g.detach().cpu().numpy()\n",
    "            indices = data_idx.detach().cpu().numpy()\n",
    "\n",
    "            probs = outputs.detach().cpu().numpy()\n",
    "        else:  # concatenate\n",
    "            acc_y_pred = np.concatenate([\n",
    "                acc_y_pred,\n",
    "                np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "            ])\n",
    "            acc_y_true = np.concatenate([acc_y_true, y.detach().cpu().numpy()])\n",
    "            acc_g_true = np.concatenate([acc_g_true, g.detach().cpu().numpy()])\n",
    "            indices = np.concatenate([indices, data_idx.detach().cpu().numpy()])\n",
    "            probs = np.concatenate([probs, outputs.detach().cpu().numpy()], axis=0)\n",
    "\n",
    "        assert probs.shape[0] == indices.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "105it [00:16,  6.45it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# part1\n",
    "with torch.set_grad_enabled(is_training):  # to make sure we don't save grad when val\n",
    "    for batch_idx, batch in tqdm(enumerate(part1_loader)):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x, y, g, data_idx = batch\n",
    "        outputs = model1(x)\n",
    "\n",
    "        # now log the desired stats\n",
    "        # Calculate stats -- get the prediction and compare with groundtruth -- save to output df\n",
    "        if batch_idx == 0:\n",
    "            acc_y_pred1 = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "            acc_y_true1 = y.detach().cpu().numpy()\n",
    "            acc_g_true1 = g.detach().cpu().numpy()\n",
    "            indices1 = data_idx.detach().cpu().numpy()\n",
    "\n",
    "            probs1 = outputs.detach().cpu().numpy()\n",
    "        else:  # concatenate\n",
    "            acc_y_pred1 = np.concatenate([\n",
    "                acc_y_pred1,\n",
    "                np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "            ])\n",
    "            acc_y_true1 = np.concatenate([acc_y_true1, y.detach().cpu().numpy()])\n",
    "            acc_g_true1 = np.concatenate([acc_g_true1, g.detach().cpu().numpy()])\n",
    "            indices1 = np.concatenate([indices1, data_idx.detach().cpu().numpy()])\n",
    "            probs1 = np.concatenate([probs1, outputs.detach().cpu().numpy()], axis=0)\n",
    "\n",
    "        assert probs1.shape[0] == indices1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average acc [n=3356]: 0.9451728247914184\n",
      "Group 0 [n=2430]: group_acc = 0.9259259259259259\n",
      "Group 1 [n=141]: group_acc = 0.9929078014184397\n",
      "Group 2 [n=38]: group_acc = 1.0\n",
      "Group 3 [n=747]: group_acc = 0.9959839357429718\n"
     ]
    }
   ],
   "source": [
    "# now calculate the final stats\n",
    "pred_acc = (acc_y_pred == acc_y_true)\n",
    "avg_acc = np.sum(pred_acc)/len(pred_acc)\n",
    "print(f\"average acc [n={len(pred_acc)}]: {avg_acc}\")\n",
    "for g in range(4):  # now calculate per-group acc\n",
    "    g_count = np.sum(acc_g_true == g)\n",
    "    group_acc = np.sum(pred_acc * (acc_g_true == g))/g_count\n",
    "    print(f\"Group {g} [n={g_count}]: group_acc = {group_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average acc [n=3356]: 1.0\n",
      "Group 0 [n=2430]: group_acc = 1.0\n",
      "Group 1 [n=141]: group_acc = 1.0\n",
      "Group 2 [n=38]: group_acc = 1.0\n",
      "Group 3 [n=747]: group_acc = 1.0\n"
     ]
    }
   ],
   "source": [
    "# now calculate the final stats for model1\n",
    "pred_acc1 = (acc_y_pred1 == acc_y_true1)\n",
    "avg_acc1 = np.sum(pred_acc1)/len(pred_acc1)\n",
    "print(f\"average acc [n={len(pred_acc1)}]: {avg_acc1}\")\n",
    "for g in range(4):  # now calculate per-group acc\n",
    "    g_count1 = np.sum(acc_g_true1 == g)\n",
    "    group_acc1 = np.sum(pred_acc1 * (acc_g_true1 == g))/g_count1\n",
    "    print(f\"Group {g} [n={g_count1}]: group_acc = {group_acc1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
