{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import run_epoch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "from utils import set_seed, Logger, CSVBatchLogger, log_args, get_model, hinge_loss, split_data, check_args, get_subsampled_indices\n",
    "import numpy as np\n",
    "\n",
    "# get path\n",
    "p = '0.7'\n",
    "dataset = 'CelebA'\n",
    "p2wd = '2e-05' if dataset == 'CUB' else '0.0001'\n",
    "seed = 0\n",
    "model_name = 'best_wg_acc_model'\n",
    "main_dir = f\"/home/thien/research/pseudogroups/{dataset}/splitpgl_sweep_logs/\" \\\n",
    "                  f\"p{p}_wd0.0001_lr0.0001\"\n",
    "best_model_path = f\"{main_dir}/part2_oll-1rw_rgl_group_dro_p0.7_wd{p2wd}_lr0.0001_s0/{model_name}.pth\"\n",
    "data_path = f\"{main_dir}/part1_s{seed}/part1and2_data_p{p}\"\n",
    "\n",
    "best_model_path1 = f\"{main_dir}/part1_s{seed}/best_model.pth\"\n",
    "device = 'cuda:0'\n",
    "\n",
    "# load data splits\n",
    "data = torch.load(data_path)\n",
    "part1_data, part2_data = data['part1'], data['part2']\n",
    "batch_size = 32\n",
    "\n",
    "part1_loader = DataLoader(part1_data, shuffle=False, batch_size=batch_size, pin_memory=True)\n",
    "part2_loader = DataLoader(part2_data, shuffle=False, batch_size=batch_size, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model = torch.load(best_model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "model1 = torch.load(best_model_path1)\n",
    "model1.to(device)\n",
    "model1.eval()\n",
    "\n",
    "\n",
    "is_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3561it [05:01, 11.81it/s]\n",
      "3561it [05:02, 11.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# now run the model on the desired dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "loader = part1_loader\n",
    "\n",
    "with torch.set_grad_enabled(is_training):  # to make sure we don't save grad when val\n",
    "    for batch_idx, batch in tqdm(enumerate(loader)):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x, y, g, data_idx = batch\n",
    "        outputs = model(x)\n",
    "\n",
    "        # now log the desired stats\n",
    "        # Calculate stats -- get the prediction and compare with groundtruth -- save to output df\n",
    "        if batch_idx == 0:\n",
    "            acc_y_pred = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "            acc_y_true = y.detach().cpu().numpy()\n",
    "            acc_g_true = g.detach().cpu().numpy()\n",
    "            indices = data_idx.detach().cpu().numpy()\n",
    "\n",
    "            probs = outputs.detach().cpu().numpy()\n",
    "        else:  # concatenate\n",
    "            acc_y_pred = np.concatenate([\n",
    "                acc_y_pred,\n",
    "                np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "            ])\n",
    "            acc_y_true = np.concatenate([acc_y_true, y.detach().cpu().numpy()])\n",
    "            acc_g_true = np.concatenate([acc_g_true, g.detach().cpu().numpy()])\n",
    "            indices = np.concatenate([indices, data_idx.detach().cpu().numpy()])\n",
    "            probs = np.concatenate([probs, outputs.detach().cpu().numpy()], axis=0)\n",
    "\n",
    "        assert probs.shape[0] == indices.shape[0]\n",
    "\n",
    "# part1\n",
    "with torch.set_grad_enabled(is_training):  # to make sure we don't save grad when val\n",
    "    for batch_idx, batch in tqdm(enumerate(loader)):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x, y, g, data_idx = batch\n",
    "        outputs = model1(x)\n",
    "\n",
    "        # now log the desired stats\n",
    "        # Calculate stats -- get the prediction and compare with groundtruth -- save to output df\n",
    "        if batch_idx == 0:\n",
    "            acc_y_pred1 = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "            acc_y_true1 = y.detach().cpu().numpy()\n",
    "            acc_g_true1 = g.detach().cpu().numpy()\n",
    "            indices1 = data_idx.detach().cpu().numpy()\n",
    "\n",
    "            probs1 = outputs.detach().cpu().numpy()\n",
    "        else:  # concatenate\n",
    "            acc_y_pred1 = np.concatenate([\n",
    "                acc_y_pred1,\n",
    "                np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "            ])\n",
    "            acc_y_true1 = np.concatenate([acc_y_true1, y.detach().cpu().numpy()])\n",
    "            acc_g_true1 = np.concatenate([acc_g_true1, g.detach().cpu().numpy()])\n",
    "            indices1 = np.concatenate([indices1, data_idx.detach().cpu().numpy()])\n",
    "            probs1 = np.concatenate([probs1, outputs.detach().cpu().numpy()], axis=0)\n",
    "\n",
    "        assert probs1.shape[0] == indices1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERM model\n",
      "Average acc [n=113939]: 0.9653\n",
      "Group 0 [n=50311]: group_acc = 0.9671\n",
      "Group 1 [n=46652]: group_acc = 0.9964\n",
      "Group 2 [n=16012]: group_acc = 0.8990\n",
      "Group 3 [n=964]: group_acc = 0.4627\n",
      "\n",
      "Retrained model\n",
      "Average acc [n=113939]: 0.9239\n",
      "Group 0 [n=50311]: group_acc = 0.9172\n",
      "Group 1 [n=46652]: group_acc = 0.9217\n",
      "Group 2 [n=16012]: group_acc = 0.9518\n",
      "Group 3 [n=964]: group_acc = 0.9180\n"
     ]
    }
   ],
   "source": [
    "print(\"ERM model\")\n",
    "pred_acc1 = (acc_y_pred1 == acc_y_true1)\n",
    "avg_acc1 = np.sum(pred_acc1)/len(pred_acc1)\n",
    "print(f\"Average acc [n={len(pred_acc1)}]: {avg_acc1:.4f}\")\n",
    "for g in range(4):  # now calculate per-group acc\n",
    "    g_count1 = np.sum(acc_g_true1 == g)\n",
    "    group_acc1 = np.sum(pred_acc1 * (acc_g_true1 == g))/g_count1\n",
    "    print(f\"Group {g} [n={g_count1}]: group_acc = {group_acc1:.4f}\")\n",
    "\n",
    "print(\"\\nRetrained model\")\n",
    "# now calculate the final stats\n",
    "pred_acc = (acc_y_pred == acc_y_true)\n",
    "avg_acc = np.sum(pred_acc)/len(pred_acc)\n",
    "print(f\"Average acc [n={len(pred_acc)}]: {avg_acc:.4f}\")\n",
    "for g in range(4):  # now calculate per-group acc\n",
    "    g_count = np.sum(acc_g_true == g)\n",
    "    group_acc = np.sum(pred_acc * (acc_g_true == g))/g_count\n",
    "    print(f\"Group {g} [n={g_count}]: group_acc = {group_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff: 7537 (6.61494308357981%)\n",
      "Group 0 [n=50311]: diff = 2609 %=0.0518574466816402\n",
      "Group 1 [n=46652]: diff = 3489 %=0.07478779044842665\n",
      "Group 2 [n=16012]: diff = 1000 %=0.062453160129902575\n",
      "Group 3 [n=964]: diff = 439 %=0.4553941908713693\n"
     ]
    }
   ],
   "source": [
    "print(f\"diff: {sum(acc_y_pred != acc_y_pred1)} ({sum(acc_y_pred != acc_y_pred1)*100/len(pred_acc)}%)\")\n",
    "for g in range(4):  # now calculate per-group acc\n",
    "    g_count = np.sum(acc_g_true == g)\n",
    "    pred = acc_y_pred * (acc_g_true == g)\n",
    "    pred1 = acc_y_pred1 * (acc_g_true1 == g)\n",
    "    print(f\"Group {g} [n={g_count}]: diff = {sum(pred != pred1)} %={sum(pred != pred1)/g_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd951f11f94420e8d595a97ad44a9b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d7a61c3d5e4c76a46f3c7f25a7a76a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now run the model on the desired dataset\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "loader = part2_loader\n",
    "\n",
    "with torch.set_grad_enabled(is_training):  # to make sure we don't save grad when val\n",
    "    for batch_idx, batch in tqdm(enumerate(loader)):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x, y, g, data_idx = batch\n",
    "        outputs = model(x)\n",
    "\n",
    "        # now log the desired stats\n",
    "        # Calculate stats -- get the prediction and compare with groundtruth -- save to output df\n",
    "        if batch_idx == 0:\n",
    "            acc_y_pred = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "            acc_y_true = y.detach().cpu().numpy()\n",
    "            acc_g_true = g.detach().cpu().numpy()\n",
    "            indices = data_idx.detach().cpu().numpy()\n",
    "\n",
    "            probs = outputs.detach().cpu().numpy()\n",
    "        else:  # concatenate\n",
    "            acc_y_pred = np.concatenate([\n",
    "                acc_y_pred,\n",
    "                np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "            ])\n",
    "            acc_y_true = np.concatenate([acc_y_true, y.detach().cpu().numpy()])\n",
    "            acc_g_true = np.concatenate([acc_g_true, g.detach().cpu().numpy()])\n",
    "            indices = np.concatenate([indices, data_idx.detach().cpu().numpy()])\n",
    "            probs = np.concatenate([probs, outputs.detach().cpu().numpy()], axis=0)\n",
    "\n",
    "        assert probs.shape[0] == indices.shape[0]\n",
    "\n",
    "# part1\n",
    "with torch.set_grad_enabled(is_training):  # to make sure we don't save grad when val\n",
    "    for batch_idx, batch in tqdm(enumerate(loader)):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x, y, g, data_idx = batch\n",
    "        outputs = model1(x)\n",
    "\n",
    "        # now log the desired stats\n",
    "        # Calculate stats -- get the prediction and compare with groundtruth -- save to output df\n",
    "        if batch_idx == 0:\n",
    "            acc_y_pred1 = np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "            acc_y_true1 = y.detach().cpu().numpy()\n",
    "            acc_g_true1 = g.detach().cpu().numpy()\n",
    "            indices1 = data_idx.detach().cpu().numpy()\n",
    "\n",
    "            probs1 = outputs.detach().cpu().numpy()\n",
    "        else:  # concatenate\n",
    "            acc_y_pred1 = np.concatenate([\n",
    "                acc_y_pred1,\n",
    "                np.argmax(outputs.detach().cpu().numpy(), axis=1)\n",
    "            ])\n",
    "            acc_y_true1 = np.concatenate([acc_y_true1, y.detach().cpu().numpy()])\n",
    "            acc_g_true1 = np.concatenate([acc_g_true1, g.detach().cpu().numpy()])\n",
    "            indices1 = np.concatenate([indices1, data_idx.detach().cpu().numpy()])\n",
    "            probs1 = np.concatenate([probs1, outputs.detach().cpu().numpy()], axis=0)\n",
    "\n",
    "        assert probs1.shape[0] == indices1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERM model\n",
      "Average acc [n=48831]: 0.9556\n",
      "Group 0 [n=21318]: group_acc = 0.9579\n",
      "Group 1 [n=20222]: group_acc = 0.9955\n",
      "Group 2 [n=6868]: group_acc = 0.8682\n",
      "Group 3 [n=423]: group_acc = 0.3546\n",
      "\n",
      "Retrained model\n",
      "Average acc [n=48831]: 0.9204\n",
      "Group 0 [n=21318]: group_acc = 0.9134\n",
      "Group 1 [n=20222]: group_acc = 0.9217\n",
      "Group 2 [n=6868]: group_acc = 0.9365\n",
      "Group 3 [n=423]: group_acc = 0.9527\n"
     ]
    }
   ],
   "source": [
    "print(\"ERM model\")\n",
    "pred_acc1 = (acc_y_pred1 == acc_y_true1)\n",
    "avg_acc1 = np.sum(pred_acc1)/len(pred_acc1)\n",
    "print(f\"Average acc [n={len(pred_acc1)}]: {avg_acc1:.4f}\")\n",
    "for g in range(4):  # now calculate per-group acc\n",
    "    g_count1 = np.sum(acc_g_true1 == g)\n",
    "    group_acc1 = np.sum(pred_acc1 * (acc_g_true1 == g))/g_count1\n",
    "    print(f\"Group {g} [n={g_count1}]: group_acc = {group_acc1:.4f}\")\n",
    "\n",
    "print(\"\\nRetrained model\")\n",
    "# now calculate the final stats\n",
    "pred_acc = (acc_y_pred == acc_y_true)\n",
    "avg_acc = np.sum(pred_acc)/len(pred_acc)\n",
    "print(f\"Average acc [n={len(pred_acc)}]: {avg_acc:.4f}\")\n",
    "for g in range(4):  # now calculate per-group acc\n",
    "    g_count = np.sum(acc_g_true == g)\n",
    "    group_acc = np.sum(pred_acc * (acc_g_true == g))/g_count\n",
    "    print(f\"Group {g} [n={g_count}]: group_acc = {group_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff: 3274 (6.704757223894657%)\n",
      "Group 0 0.4366 [n=21318]: diff = 1011 %=0.04742471151139882\n",
      "Group 1 0.4141 [n=20222]: diff = 1493 %=0.07383048165364454\n",
      "Group 2 0.1406 [n=6868]: diff = 517 %=0.07527664531158998\n",
      "Group 3 0.0087 [n=423]: diff = 253 %=0.5981087470449172\n"
     ]
    }
   ],
   "source": [
    "print(f\"diff: {sum(acc_y_pred != acc_y_pred1)} ({sum(acc_y_pred != acc_y_pred1)*100/len(pred_acc)}%)\")\n",
    "for g in range(4):  # now calculate per-group acc\n",
    "    g_count = np.sum(acc_g_true == g)\n",
    "    pred = acc_y_pred * (acc_g_true == g)\n",
    "    pred1 = acc_y_pred1 * (acc_g_true1 == g)\n",
    "    print(f\"Group {g} {g_count/len(pred_acc):.4f} [n={g_count}]: diff = {sum(pred != pred1)} %={sum(pred != pred1)/g_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
